
# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘01
https://youtu.be/Ye018rCVvOo?si=cKmhbW6FT60eZH-F
æœ¬ä¾†å°±éƒ½æœƒäº†, æ‡¶å¾—åšç­†è¨˜.
# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘02 # é æ¸¬æœ¬é »é“è§€çœ‹äººæ•¸ (ä¸‹) - æ·±åº¦å­¸ç¿’åŸºæœ¬æ¦‚å¿µç°¡ä»‹
https://youtu.be/bHcJCp2Fyxs?si=uSrPpHlkR3J96ak3

æœ¬å ‚èª²ä¸€åœ–ä»¥è“‹ä¹‹:
![[Pasted image 20250915154720.png]]

Q: è€å¸«åœ¨Slideçš„P23~26ä½¿ç”¨äº†HandWavingçš„æ–¹å¼è­‰æ˜äº†ä»»ä½•ä¸€ç¶­çš„é€£çºŒå‡½æ•¸éƒ½èƒ½ç”¨å¦‚ä¸‹çš„HardSigmoid functionä¾†è¿‘ä¼¼:
$$
\begin{aligned}
& y=b+\sum_i c_i \operatorname{HardSigmoid}\left(\underline{b_i+w_i x_1}\right) \\
\end{aligned}
$$
ä½†æ˜¯å¾Œé¢(P30)ç›´æ¥å»¶ä¼¸åˆ°nç¶­æ²’çµ¦è­‰æ˜ï¼Œè©¦è‘—è‡ªå·±è­‰æ˜nç¶­çš„é€£çºŒå‡½æ•¸å¦‚ä½•èƒ½è¢«ä¸‹é¢çš„å‡½ç¤ºç–ŠåŠ è¿‘ä¼¼? Hint: å…ˆæƒ³åœ¨2ç¶­ï¼ŒLocallyä»»æ„çš„functionæ˜¯å¦å¯ä»¥è¢«è¿‘ä¼¼? å‰Šè˜‹æœæ³•~
$$
\begin{aligned}
& y=b+\sum_i c_i \operatorname{HardSigmoid}\left(\underline{b_i+\sum_j w_{i j} x_j}\right)
\end{aligned}
$$

Ref: P30æŠŠModelå¾1ç¶­å»¶ä¼¸åˆ°nç¶­å¦‚ä¸‹:
$$
\begin{aligned}
&\text { New Model: More Features }\\
&\begin{aligned}
& y=\underline{b+w x_1} \\
& y=b+\sum_i c_i \operatorname{sigmoid}\left(\underline{b_i+w_i x_1}\right) \\
& y=\underline{b+\sum_j w_j x_j} \\
& y=b+\sum_i c_i \operatorname{sigmoid}\left(\underline{b_i+\sum_j w_{i j} x_j}\right)
\end{aligned}
\end{aligned}
$$
P44 Batch, Epoch: Updateæ‰€æœ‰Batchä¸€æ¬¡. Update: Batch Sizeä¹Ÿæ˜¯ä¸€å€‹HyperParameter.
P47~48 
Rectified Linear Unit (ReLU): 
![[Pasted image 20250914194806.png]]
Activation function: sigmoid, HardSigmoid, ä½¿ç”¨è€…è‡ªç”±æ±ºå®š.
Neuron, Neuron Network, hidden layer, Deep Learning: Deep = Many hidden layers.
P57 Why we want â€œDeepâ€ network, not â€œFatâ€ network? (æœªä¾†èª²ç¨‹å›ç­”)

èª²å¾Œé–±è®€:
Basic Introduction: èˆŠç‰ˆçš„Deep Learningä»‹ç´¹
https://www.youtube.com/watch?v=Dr-WRlEFefw
Backpropagation: Computing gradients in an efficient way
https://www.youtube.com/watch?v=ibJpTrp5mcE

Q: æ¯ä¸€å±¤éƒ½æ˜¯æŠŠè©²å±¤çš„aæ‹¿å»è·Ÿyç®—Loss(a, y)ä¾†é€å±¤çš„æ±ºå®š(b, w)çš„æœ€ä½³åŒ–å€¼å—? é‚„æ˜¯æ¯ä¸€æ¬¡updaterå°±åŒæ™‚æ›´æ–°äº†æ‰€æœ‰å±¤çš„(b, w)
![[Pasted image 20250914195912.png]]

# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘03 # æ©Ÿå™¨å­¸ç¿’ä»»å‹™æ”»ç•¥
https://youtu.be/WeHM2xpYQpw?si=GdNzJ2wa5rqBb355

æœ¬èª²ç¨‹: æŠŠLearningåšå¥½çš„ä¸€äº›åŸºæœ¬æ‹›å¼
Losså¤ªé«˜ => è¾¨åˆ¥å•é¡Œç™¥çµ:
Model Bias: Modelä¸å¤ è¤‡é›œ => æŠŠModelè®Šå¾—æ›´å¤§ã€‚
Optimization: Modelå·²ç¶“è¶³å¤ è¤‡é›œï¼Œæ˜¯æ‰¾æœ€ä½³è§£çš„æ™‚å€™å‡ºå•é¡Œï¼Œä¾‹å¦‚å¯èƒ½æ˜¯Gradient Descenté€™å€‹æ–¹æ³•å¡åœ¨Local Minimumçš„å•é¡Œã€‚
Tips: ä»»ä½•å•é¡Œéƒ½å…ˆå¾LRæˆ–æ˜¯SVMé–‹å§‹(æ¯”è¼ƒæ²’æœ‰æœ€ä½³åŒ–å•é¡Œ)ï¼Œå†æ›æˆè¤‡é›œçš„modelï¼Œè·Ÿå‰é¢çš„åˆéšé€šç”¨Modelæ¯”è¼ƒï¼Œçµæœå¦‚æœæ¯”è¼ƒå·®ï¼Œå°±å¯ä»¥æ¨æ¸¬è¤‡é›œçš„modelå¯èƒ½æ˜¯æœ‰æœ€ä½³åŒ–å•é¡Œã€‚


Data augmentation: å¢åŠ (äººå·¥ç”¢ç”Ÿçš„)è³‡æ–™ã€‚éœ€è¦æœ‰Domain Knowledgeç•¶guideï¼Œä¾†ç”¢ç”Ÿåˆç†çš„
fakeè³‡æ–™ã€‚
constrained model: (æ ¹æ“šDomain Knowledge)é™åˆ¶ä½ çš„modelè‡ªç”±åº¦: 
â€¢ç‰¹å®šå½¢å¼çš„function â€¢Less parameters, sharing parameters
â€¢ Less features â€¢ Early stopping â€¢ Regularization â€¢ Dropout

Validation:  N-fold Cross Validation
 Mismatch: Training data è·Ÿ Testing dateçš„åˆ†å¸ƒä¸ä¸€æ¨£ã€‚HW11
# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘04~05 # # é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ï¼š å±€éƒ¨æœ€å°å€¼ (local minima) èˆ‡éé» (saddle point) / æ‰¹æ¬¡ (batch) èˆ‡å‹•é‡ (momentum)
https://youtu.be/QW6uINn7uGk?si=DLGu8ZkFLmWX2N_6
https://youtu.be/zzbr1h9sF54?si=BrYqoKnnigO_mrxB
Critical point: Gradient = 0 => Saddle point or local minimum maximum.
æª¢æŸ¥Hessiançš„eigen valueä¾†åˆ¤æ–·æ˜¯ä½•ç¨®ã€‚
ç®—å‡ºHessian => æ²¿è‘—è² Eigenvalueçš„Eigenvector => Lossè®Šå°ã€‚ä½†å¯¦å‹™ä¸ŠHessianå› ç‚ºè¨ˆç®—é‡å¤§ï¼Œé€šå¸¸æ²’åœ¨ç”¨ã€‚
ç ”ç©¶ä¸Šç™¼ç¾å› ç‚ºModelçš„åƒæ•¸/ç¶­åº¦å¾ˆå¤§ï¼Œé€šå¸¸éƒ½æ˜¯å¡åœ¨SaddlePointè€Œä¸æ˜¯LocalMinimumã€‚ 

Smaller batch size and momentum help escape critical points:
Batch:
small Batch => Optimization(training data)ï¼ŒGeneralization(test data)æ¯”è¼ƒå¥½!
 => (Training) Time for one epoch æ¯”è¼ƒå¿«!
èª²å¾Œé–±è®€: slideä¸Šciteä¸€äº›ç ”ç©¶ï¼Œé‹ç”¨ä¸€äº›æŠ€å·§ä¾†ä½¿ç”¨large Batchä¸¦ä¸”é¿å…large Batchçš„åŠ£å‹¢ã€‚

Momentum:
## (Vanilla) Gradient Descent:
![[Pasted image 20250913162615.png]]
Starting at $\boldsymbol{\theta}^{\mathbf{0}}$
Compute gradient $\boldsymbol{g}^{\mathbf{0}}$
Move to $\boldsymbol{\theta}^{\mathbf{1}}=\boldsymbol{\theta}^{\mathbf{0}}-\eta \boldsymbol{g}^{\mathbf{0}}$
Compute gradient $\boldsymbol{g}^{\mathbf{1}}$
Move to $\boldsymbol{\theta}^{\mathbf{2}}=\boldsymbol{\theta}^{\mathbf{1}}-\eta \boldsymbol{g}^{\mathbf{1}}$

## Gradient Descent + Momentum:
![[Pasted image 20250913162740.png]]
Starting at $\boldsymbol{\theta}^{\mathbf{0}}$
Movement $\boldsymbol{m}^{\mathbf{0}}=\mathbf{0}$
Compute gradient $\boldsymbol{g}^{\mathbf{0}}$
Movement $\boldsymbol{m}^{\mathbf{1}}=\lambda \boldsymbol{m}^{\mathbf{0}}-\eta \boldsymbol{g}^{\mathbf{0}}$
Move to $\boldsymbol{\theta}^{\mathbf{1}}=\boldsymbol{\theta}^{\mathbf{0}}+\boldsymbol{m}^{\mathbf{1}}$
Compute gradient $\boldsymbol{g}^{\mathbf{1}}$
Movement $\boldsymbol{m}^{\mathbf{2}}=\lambda \boldsymbol{m}^{\mathbf{1}}-\eta \boldsymbol{g}^{\mathbf{1}}$
Move to $\boldsymbol{\theta}^{\mathbf{2}}=\boldsymbol{\theta}^{\mathbf{1}}+\boldsymbol{m}^{\mathbf{2}}$
Movement not just based on gradient, but previous movement.

# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘06~ # # é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ï¼šè‡ªå‹•èª¿æ•´å­¸ç¿’é€Ÿç‡ (Learning Rate)

https://youtu.be/HYUXEeh3kwY?si=STswcAHcyFFZIRhL


Gradient Descentå¾€å¾€å¾ˆé›£èµ°åˆ°norm of Gradientå¾ˆæ¥è¿‘0çš„ä½ç½®(critical point)ã€‚
GD -> RMSProp -> Adam = RMSProp + Momentum -> + Learning Rate Scheduling
Learning Rate Scheduling: $\boldsymbol{\theta}_i^{\boldsymbol{t}+\boldsymbol{1}} \leftarrow \boldsymbol{\theta}_i^{\boldsymbol{t}}-\frac{\boldsymbol{\eta^t}}{\sigma_i^t} \boldsymbol{g}_i^{\boldsymbol{t}}$
A. Learning Rate Decay: æœ€å¾Œå·²ç¶“æ¥è¿‘çµ‚é»æ™‚å¯ä»¥èµ°æ…¢é»ï¼Œé¿å…æœ€å¾Œéœ‡ç›ªã€‚
![[Pasted image 20250913221016.png]]
B. Warm Up: æœ‰æ•ˆçš„åŸå› å¾…ç ”ç©¶ã€‚å¯èƒ½çš„è§£é‡‹: ä¸€é–‹å§‹ç”¨ä¾†è¨ˆç®—$\sigma$çš„é»ä¸å¤ å¤šï¼Œçµ±è¨ˆèª¤å·®å¤§ã€‚ç­‰ç¶“éçš„é»å¤šäº†ï¼Œè¨ˆç®—å‡ºä¾†çš„$\sigma$çµ±è¨ˆä¸Šæ›´å¯é æ™‚å†èµ°å¿«ä¸€é»ã€‚
![[Pasted image 20250913221025.png]]
èª²å¾Œé–±è®€: æ›´å¤šé—œæ–¼çš„Optimizationæ±è¥¿
[TA è£œå……èª²] Optimization for Deep Learning
https://www.youtube.com/watch?v=4pUmZ8hXlHM
https://www.youtube.com/watch?v=e03YKGHXnL8
## My Reflections
**GD**:  $\theta_{t+1}=\theta_t-\eta \nabla_\theta J(\theta)=\theta_t-\eta g_t$
å¦‚æœæ¢¯åº¦å¤§ â†’ èµ°å¾—å¿«ï¼›æ¢¯åº¦å° â†’ èµ°å¾—æ…¢ã€‚ç¼ºé»ï¼šæ¢¯åº¦å°æ™‚æœƒèµ°å¾—æ…¢ï¼Œç„¡æ³•æ”¶æ–‚åˆ°Local minimum(ä¸ç¢ºå®š?)
![[Pasted image 20250913214201.png]]
**NGD**(Normalized Gradient Descent): $\theta_{t+1}=\theta_t-\eta \frac{g_t}{\left\|g_t\right\|}$
ä¸ç®¡æ¢¯åº¦å¤šå¤§ï¼Œæ°¸é èµ°ã€Œå›ºå®šé•·åº¦ã€çš„ä¸€æ­¥ã€‚ã€‚å„ªé»ï¼šé¿å…æ¢¯åº¦çˆ†ç‚¸ã€‚ç¼ºé»ï¼šå¿½ç•¥æ¢¯åº¦å¤§å°ï¼Œæ”¶æ–‚æ…¢ï¼Œå¯èƒ½æœƒåœ¨è°·åº•å…©å´éœ‡ç›ª(?)
**RMSProp**: $\boldsymbol{\theta}_i^{\boldsymbol{t}+\boldsymbol{1}} \leftarrow \boldsymbol{\theta}_i^{\boldsymbol{t}}-\frac{\eta}{\sigma_i^t} \boldsymbol{g}_i^{\boldsymbol{t}} \quad \sigma_i^t=\sqrt{\alpha\left(\sigma_i^{t-1}\right)^2+(1-\alpha)\left(\boldsymbol{g}_i^{\boldsymbol{t}}\right)^2}$

Q: ç„¡æ³•ç†è§£é€™æ¨£ç‚ºä»€éº¼æ¯”è¼ƒå¥½ã€‚ç‚ºä»€éº¼ä¸æ˜¯ç”¨æ›´ç›´è§€çš„å…¨åŸŸä¿®æ­£çš„æ–¹å¼å¦‚ä¸‹:
$\theta_i^{t+1} \leftarrow \theta_i^t-\frac{\eta}{\sigma^t}, \sigma^t=\sqrt{\alpha\left\|\sigma^{t-1}\right\|^2+(1-\sigma)\left\|g^t\right\|^2}$
ç°¡è¨€ä¹‹ï¼Œå…¨åŸŸä¿®æ­£($\left\|\boldsymbol{g}^{t-1}\right\|^2$) vs. per-parameter($(g_i^{\boldsymbol{t}})^2$)) ä¿®æ­£: è®“å€‹åˆ¥çš„parameterè‡ªå·±å»èª¿æ•´ç‚ºä½•æ¯”è¼ƒå¥½?

Momentum: $v_t=\beta v_{t-1}+(1-\beta) g_t, \quad \theta_{t+1}=\theta_t-\eta v_t$
é€™å€‹ç‰ˆæœ¬æ¯”è€å¸«æ•™çš„å¤šäº†ä¸€å€‹$\beta$è‡ªç”±åº¦ï¼Œè¦ºå¾—æ‡‰è©²æ˜¯æ¯”è¼ƒå¥½çš„ç‰ˆæœ¬ã€‚

# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘07 # é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ (å››)ï¼šæå¤±å‡½æ•¸ (Loss) ä¹Ÿå¯èƒ½æœ‰å½±éŸ¿
https://youtu.be/O2VkP8dJ5FE?si=3p9YEgszC-8Pf1U8

å»ºè­°é–±è®€éå»æ›´å®Œæ•´ç‰ˆçš„Classificationæ•™å­¸:
https://youtu.be/fZAZUYEeIMg?si=rzaC3rghEbkU201N
https://youtu.be/hSXFuypLukA?si=62MqJx02mXiy-CkV

**x**ç¶“NNs(Neural Network)å¾—åˆ°**y**(å¯¦æ•¸)ï¼Œå†è¿½åŠ ç¶“éä¸€å±¤softmaxå¾—åˆ°y'(æ©Ÿç‡å‘é‡)
ä¹‹å¾Œé€™è£¡å®šç¾©Loss as Cross-entropy: $e=-\sum_i \widehat{\boldsymbol{y}}_i \ln \boldsymbol{y}_i^{\prime}$
æœƒæ¯”MSE(Mean Square Error)æ›´å¥½ã€‚æ™‚é–“æœ‰é™ï¼Œèª²ç¨‹ä¸­åªçµ¦äº†Handwavingçš„è§£é‡‹ã€‚
# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘08 é¡ç¥ç¶“ç¶²è·¯è¨“ç·´ä¸èµ·ä¾†æ€éº¼è¾¦ (äº”)ï¼š æ‰¹æ¬¡æ¨™æº–åŒ– (Batch Normalization) ç°¡ä»‹
https://youtu.be/BABPWOkSbLE?si=Nv7ldK5z5lrZZf5I

Batch normalization: (æ¯å€‹batché€²è¡Œè¨ˆç®—æ™‚)å°æ¯ä¸€å±¤çš„input xiè·ŸActivation functionå‰(æˆ–å¾Œ)çš„åƒæ•¸å„åšä¸€æ¬¡standard normalization. è‹¥Activation functionæ˜¯sigmoid, å»ºè­°è¦åœ¨Activation functionå‰çš„åƒæ•¸åšNormalizationæ¯”è¼ƒå¥½.
Inference: å°±æ˜¯testing.
Batch normalization â€“ Testing: testing dataè¦ä¸€ç­†ä¸€ç­†å³æ™‚ç®—, æ²’æœ‰batch, ç„¡æ³•è¨ˆç®—(Ïƒ, Âµ). Computing the moving average of ğ and ğˆ of the batches during training ç”¨ä¾†çµ¦testing dataä½¿ç”¨.
å»¶ä¼¸é–±è®€: Batch normalizationå¦‚ä½•ç”¨åœ¨CNNä¸Š? åƒè€ƒOriginal paper: https://arxiv.org/abs/1502.03167

Q: è½ç„¡ï¼Œé€™æ®µçš„(Î³, Î²)æ˜¯å•¥? æ€éº¼æ±ºå®š? ç‚ºä»€éº¼ä¸æ˜¯ç”¨åŸæœ¬çš„(Ïƒ, Âµ) (layer by layer çš„)å›æ¨å›ç¬¬ä¸€å±¤?
https://youtu.be/BABPWOkSbLE?list=PLJV_el3uVTsMhtt7_Y6sgTHGHp1Vb2P2J&t=1083
$$
\begin{aligned}
& \tilde{z}^i=\frac{z^i-\mu}{\sigma} \\
& \hat{z}^i=\gamma \odot \tilde{z}^i+\beta
\end{aligned}
$$

To learn more: (about Renormalization)
â€¢ Batch Renormalization â€¢ https://arxiv.org/abs/1702.03275 
â€¢ Layer Normalization â€¢ https://arxiv.org/abs/1607.06450 
â€¢ Instance Normalization â€¢ https://arxiv.org/abs/1607.08022 
â€¢ Group Normalization â€¢ https://arxiv.org/abs/1803.08494
â€¢ Weight Normalization â€¢ https://arxiv.org/abs/1602.07868 
â€¢ Spectrum Normalization â€¢ https://arxiv.org/abs/1705.10941

# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘09 å·ç©ç¥ç¶“ç¶²è·¯ (Convolutional Neural Networks, CNN)
https://youtu.be/OP5HcXJg2Aw?si=FuiWNu2ARWH7oCDT

CNN: ç‰¹åŒ–ä¾†å½±åƒè™•ç†çš„.
æœ¬èª²ç¨‹ç”¨å…©å€‹è§€é»å»ç†è§£CNN
Neuron Version Story: Each neuron only considers a receptive field. The neurons with different receptive fields share the parameters.
Filter Version Story: There are a set of filters detecting small patterns. Each filter convolves over the input image.
PS: Neuron Version Storyä¸­ä¸€çµ„sharing parameterå°±æ˜¯Filter Version Storyä¸­çš„ä¸€å€‹filter!

Fully Connected Network: æ¯å€‹Neuronéƒ½è·Ÿinputçš„æ¯ä¸€å€‹dimensionæœ‰ä¸€å€‹weight(é€£æ¥)
Observation: A neuron does not have to see the whole image (to identify a pattern).
Receptive field: æ¯å€‹neuronåªç®¡ä¸€å€‹æœ‰é™çš„ç¯„åœ(Receptive field). ä¸åŒneuronçš„Receptive fieldå¯ä»¥é‡è¤‡ï¼Œå¯ä»¥å…¨åŒ. æ¯å€‹neuronçš„Receptive fieldé€šå¸¸æ˜¯é€£æ¥çš„ç¯„åœ, å¦‚æœç¬¦åˆä½ çš„éœ€æ±‚ä¹Ÿå¯ä»¥ç”¨ä¸é€£æ¥çš„ç¯„åœ.
Typical Setting: ç®¡è‰²å½©çš„3-channelsç¸½æ˜¯å…¨éƒ¨åŒ…å«. è‡ªç”±åº¦: kernel size(é•·å¯¬), stride(å¹³ç§»é‡), padding(é‚Šç·£è£œå€¼: çµ¦è™•ç†åˆ°æœ€é‚Šç·£çš„Receptive fieldä½¿ç”¨). parameter sharing:  ä¸åŒçš„Receptive fieldå…±äº«å…¨åŒåƒæ•¸, æ¦‚å¿µä¸Šä»£è¡¨åœ¨ä¸åŒå€åŸŸåµæ¸¬ç›¸åŒçš„pattern, Two neurons with the same receptive field would not share parameters.

Fully Connected Layer -> ç°¡åŒ–è‡ªç”±åº¦: Receptive Field + Parameter Sharing = Convolutional Layer.
CNN(Convolutional Layer Network) 

Pooling: åˆªé™¤å¶æ•¸è¡Œçš„columnè·Ÿå¥‡æ•¸è¡Œçš„row, ç”¨ä¾†æŠŠåœ–ç¸®å°(æˆ1/4), æ¸›å°‘è¨ˆç®—é‡, ä½†æ˜¯æœƒå½±éŸ¿performance. Max/min/mean Pooling. è¿‘å¹´ä¾†å¦‚æœè¨ˆç®—èƒ½åŠ›å¤ , ä¹Ÿå¯ä»¥ä¸ç”¨. optional.

Alpha Go: ä½¿ç”¨CNN, ä½†æ²’æœ‰ä½¿ç”¨Pooling! å› ç‚ºPoolingä¸é©åˆåœæ£‹.

 CNN is not invariant to scaling and rotation (we need data augmentation â˜º)! CNNçš„æ¶æ§‹, ä¸æ”¯æ´æ”¾å¤§ç¸®å°æ—‹è½‰(ä½†å¹³ç§»å¯).  Why? æœ‰ç©ºå¤šæƒ³æƒ³.

 To learn more: 
Spatial Transformer Layer:  invariant to scaling and rotation.
https://www.youtube.com/watch?v=SoCywZ1hZak


# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘10 # è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ (Self-attention) (ä¸Š)
https://youtu.be/hYdO9CscNes?si=W0UyrZPFuC5nw3NH

One-hot Encoding
![[Pasted image 20250914223213.png]]

Word Embedding
![[Pasted image 20250914223230.png]]
 To learn more: **Word Embedding** https://youtu.be/X7PH3NuYW0Q


â€¢ Each vector has a label. (focus of this lecture)
â€¢ The whole sequence has a label. (HW4)
â€¢ Model decides the number of labels itself. (HW5)


æ³•1: æœ€é™½æ˜¥, ç„¡æ³•è§£æ±º "I saw a saw" çš„labelè©æ€§å•é¡Œ.
![[Pasted image 20250914225615.png]]

æ³•2: window. ä½ ä¸å®¹æ˜“çŸ¥é“windowå¤§å°è¶³å¤ å¤§æ˜¯èªªå¤šå¤§. å¦‚æœwindowè¨­ç‚ºæ•´å€‹sequenceè¨ˆç®—é‡å¤ªå¤§, ä¸”å®¹æ˜“overfitting.
![[Pasted image 20250914225842.png]]

æ³•3: Self-attention. å…ˆå¾æ•´å€‹sequence of vector, ç®—å‡ºä¸€æ¨£å¤§å°çš„sequence of vector. é€™æ¨£å¯ä»¥çœ‹éæ•´å€‹sequenceè³‡è¨Š.
![[Pasted image 20250914230112.png]]
è¨ˆç®—ç´°ç¯€:
è€å¸«èªªä¹Ÿä¸ä¸€å®šè¦ç”¨Dot-product, å¯ä»¥ç”¨å…¶ä»–é¸æ“‡. èª²å ‚ä¸Šèˆ‰ä¾‹äº†å¦ä¸€ç¨®Additveé¸æ“‡. (ä¸éæˆ‘è¦ºå¾—è¦æ¯”è¼ƒå…©å€‹å‘é‡çš„ç›¸ä¼¼ç¨‹åº¦, é‚„æ˜¯ç”¨Dotæ¯”è¼ƒç›´è¦º.)
![[Pasted image 20250914230624.png]]
![[Pasted image 20250914230610.png]]
![[Pasted image 20250914230823.png]]

å¸«è¬›è§£: å› ç‚º$\alpha$æ˜¯æ©Ÿç‡å‘é‡, æœ€å¾Œbçš„å€¼æœƒè¶¨è¿‘æ©Ÿç‡dominantå°æ‡‰çš„é‚£å€‹$v_i$åˆ†é‡.
Q: æœ€å¾Œå¾vç®—bçš„æ„ç¾©æ˜¯ä»€éº¼? åˆ°åº•æ˜¯åœ¨å¹¹å˜›?
# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘11 # è‡ªæ³¨æ„åŠ›æ©Ÿåˆ¶ (Self-attention) (ä¸‹)
https://youtu.be/gmsMY5kc-zw?si=NhomTXgXqPXk6kXe

Multi-head Self-attention: ç‚ºäº†ç®—$q^{q,1}$, $q^{q,2}$, é™¤äº†åŸæœ¬çš„$W^q$é‚„å¤šäº†$W^{q,1}$, $W^{q,2}$(èˆ‰2headç‚ºä¾‹)
![[Pasted image 20250915011136.png]]


Q: ç‚ºä»€éº¼ä¸ä¹¾è„†æ”¹æˆä¸‹é¢é€™ç¨®æ¶æ§‹, é€™æ¨£ç®—$q^{q,1}$, $q^{q,2}$, å°±åªéœ€è¦$W^{q,1}$, $W^{q,2}$. é€™æ¨£ä¸æ˜¯æ¯”è¼ƒç°¡æ½”, è€Œä¸”modelè·Ÿä¸Šé¢çš„æ¶æ§‹æ˜¯ç­‰åƒ¹çš„ä¸æ˜¯å—?
![[Pasted image 20250915011644.png]]


Positional Encoding: ç‚ºäº†å¢åŠ ä½ç½®è³‡è¨Š, å¤šåŠ äº†ä¸€å€‹hand-crafted positional vector $e^i$
![[Pasted image 20250915012151.png]]


Q: å¦‚æœæ˜¯æˆ‘çš„è©±, æœƒç›´è§€çš„æ”¹åœ¨dot product, æŠŠ$q^i k^{i'}$é­”æ”¹åŠ å…¥depend on å…©é»è·é›¢çš„decay multiplier, åƒæ˜¯ $q^i k^{i'} e^{-|i-i'|}$. (ç›´è¦º: è·é›¢è¶Šé é—œä¿‚è¶Šå°.) ä½†é€™å€‹æ–¹æ³•åªèƒ½å¼•é€²ç›¸å°è·é›¢çš„è³‡è¨Š, ä¸åƒPositional Encodingå¯ä»¥å¼•é€²çµ•å°è·é›¢çš„è³‡è¨Š.
TODO: æœªä¾†å¯ä»¥å˜—è©¦å¯¦è¸æˆ‘çš„ä¸Šè¿°åšæ³•, çœ‹åœ¨æŸäº›å•é¡Œä¸Šæœƒä¸æœƒè®Šå¾—æ¯”è¼ƒå¥½. ä¹Ÿå¯ä»¥æŠŠé€™å€‹ä½œæ³•ç”¨åœ¨ä¸‹é¢çš„"æŠŠSelf-Attentionç”¨åœ¨å½±åƒè™•ç†"çš„å•é¡Œè©¦é©—çœ‹çœ‹æ•ˆæœ. (æ­¤æ™‚, decay depend on å…©å€‹pixelåœ¨åœ–ä¸Šçš„äºŒç¶­è·é›¢)

Truncated Self-attention: è™•ç†åƒæ˜¯èªéŸ³é€™ç¨®vectoræ•¸é‡å¾ˆå¤§çš„data, è€ƒæ…®å…¨éƒ¨è¨ˆç®—é‡æœƒå¤ªå¤§, æ”¹æˆäººç‚ºçš„åˆ‡æ®µåªè€ƒæ…®å±€éƒ¨.

Self-Attention GAN(https://arxiv.org/abs/1805.08318): æŠŠSelf-Attentionç”¨åœ¨å½±åƒè™•ç†, æ¯ä¸€å€‹pixelç•¶ä½œä¸€å€‹é•·åº¦3(RGB é‚£å€‹color channel)çš„vector.

Self-Attention: æ¯ä¸€å€‹pixelè€ƒæ…®è·Ÿå…¶ä»–æ‰€æœ‰pixelä¹‹é–“çš„ç›¸é—œè³‡è¨Š. å®¹æ˜“overfit, ä½†dataé‡å¤§æ™‚å¯ä»¥åšå¾—æ¯”CNNå¥½.
CNN: åªè€ƒæ…®åŒä¸€å€‹receptive fieldè£¡çš„pixelçš„è³‡è¨Š. Good for less data.
=> CNN æ˜¯ç°¡åŒ–ç‰ˆçš„ self-attention.
äº‹å¯¦ä¸Š, å¯ä»¥è­‰æ˜CNNæ˜¯Self-Attentionçš„ç‰¹ä¾‹.
On the Relationship between Self-Attention and Convolutional Layers https://arxiv.org/abs/1911.03584

(hw4)  è¦ç”¨CNNé‚„æ˜¯Self-Attentionå‘¢? è€å¸«æç¤ºè¦ç”¨conformer(Q: é€™å•¥?), æ—¢æœ‰ç”¨åˆ°CNNä¹Ÿæœ‰ç”¨åˆ°Self-Attention.


Self-attention v.s. RNN: 
RNN: å› ç‚ºè¿‘ä¾†é€æ¼¸è¢«Self-attentionå–ä»£, é€™é–€èª²å°±åªåœ¨é€™è£¡ç°¡çŸ­æä¸€ä¸‹è·ŸSelf-attentionåšæ¯”è¼ƒ, æœªä¾†ä¸æœƒå†æ.
ç¼ºé»: ä¸­é–“é‚£å€‹æ¶æ§‹ä¸èƒ½å¹³è¡Œè™•ç†. å–®å‘çš„RNNæ¯å€‹ç¯€é»åªèƒ½è€ƒæ…®ä¹‹å‰ç¯€é»çš„è³‡è¨Š, ä¸åƒSelf-attentionå¯ä»¥è€ƒæ…®æ•´å€‹sequenceçš„è³‡è¨Š, é›–ç„¶é›™å‘ç‰ˆæœ¬çš„RNNå¯ä»¥åŒæ™‚è€ƒæ…®å‰å¾Œ, ä½†æ˜¯èˆ‰æ¥µç«¯ä¾‹å­, æœ€å·¦é‚Šç¯€é»çš„è³‡è¨Šè¦å¾æœ€å·¦çš„memoryé–‹å§‹, ä¸€å±¤å±¤çš„å¾€å³å‚³éä¸¦ä¸”ä¸å¤±å»è³‡è¨Š, æ‰èƒ½è®“æœ€å³é‚Šçš„ç«¯é»ç²å¾—è³‡è¨Š. (Reflections: é€™è·Ÿæˆ‘ä¸Šé¢çš„TODOè£¡æƒ³åƒçš„è·é›¢decayä¹Ÿæœ‰æœ‰é›·åŒä¹‹è™•?)

![[Pasted image 20250915151822.png]]

To learn more: Self-attention åŠ ä¸Šä¸€äº›ä»€éº¼å°±æœƒè®Šæˆ RNN. https://arxiv.org/abs/2006.16236

To learn more: RNNéå»çš„ä¸Šèª²æ•™å­¸. (å› ç‚ºè€å¸«å·²ç¶“è¬›Self-attention Win! æ‰€ä»¥å»ºè­°æš«æ™‚å¯ä»¥å…ˆä¸çœ‹)
https://www.youtube.com/watch?v=xCGidAeyS4M
https://www.youtube.com/watch?v=Jjy6ER0bHv8

Self-attention for Graph: æŠŠCNNç”¨åœ¨graphä¸Šé¢æ˜¯æŸä¸€ç¨®è®Šå½¢çš„GNN
Attention Matrixçš„è³‡è¨Šä¸éœ€è¦ç”¨learnçš„å¾—åˆ°, å¯ä»¥ç›´æ¥æ¡ç”¨edgeæä¾›çš„è³‡è¨Š, æ¯å€‹ç¯€é»åªè€ƒæ…®æœ‰é€£æ¥çš„é»çš„ç›¸é—œè³‡è¨Šå³å¯. (Reflection: æ˜¯ä¸æ˜¯å¯ä»¥ä¾ç…§é€£æ¥æ·±åº¦å»åˆ†å±¤è€ƒæ…®, è®“é€£æ¥è·é›¢ä¸åŒçš„ç¯€é»é‡è¦æ€§å¯ä»¥ä¸ä¸€æ¨£? Ex: è€ƒæ…®ä¸‹åœ–é»1, æœ‰ä¸€å€‹"1-å±¤Attention Matrix": è€ƒæ…®é»5,6,8, æœ‰å¦ä¸€å€‹"2-å±¤Attention Matrix": å¤šè€ƒæ…®é»3,4,7...")
![[Pasted image 20250915153030.png]]

To learn more: GNN
https://www.youtube.com/watch?v=eybCCtNKwzA
https://www.youtube.com/watch?v=M9ht8vsVEw8

å»£ç¾©çš„Transformerå°±æ˜¯Self-attention: å¾ˆå¤šäººåœ¨è¬›Transformeræ™‚å°±æ˜¯åœ¨èªªSelf-attention.
Self-attentionæœ‰å¾ˆå¤šè®Šå½¢éƒ½æœƒè¢«å‘½åæˆXX-former.
To learn more: (çŸ­æœŸä¸ç”¨çœ‹, æœ‰èˆˆè¶£å†èªª.) Self-attentionå¦‚ä½•åšå¾—æ›´å¿«æ›´å¥½ä»ç„¶å°šå¾…ç ”ç©¶. Efficient Transformers: A Survey https://arxiv.org/abs/2009.06732

# ã€æ©Ÿå™¨å­¸ç¿’2021ã€‘12 Transformer (ä¸Š)
https://youtu.be/n9TlOhRjYoc?si=6Zt1DVhPFCv9xw7F